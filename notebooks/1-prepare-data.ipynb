{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tokenize_uk\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings and useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config(filename: Path) -> Dict:\n",
    "    with open(filename, 'r') as file:\n",
    "        config = yaml.load(file, Loader=yaml.FullLoader)\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH2ROOT = Path('..')\n",
    "PATH2CONFIG = Path(PATH2ROOT / 'configs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = get_config(PATH2CONFIG / 'config.yml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH2COURPUS = Path(PATH2ROOT / CONFIG['data']['path_to_corpus_folder'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUNCTUATION = string.punctuation + '«' + '»' + '–'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filenames() -> Dict[str, Dict[str, str]]:\n",
    "    result = dict()\n",
    "    for filename in os.listdir(PATH2COURPUS):\n",
    "        file_tokenize = filename.split('.')\n",
    "\n",
    "        if file_tokenize[-2] != 'tok':\n",
    "            continue\n",
    "        elif file_tokenize[-1] == 'ann':\n",
    "            t = ' '.join(file_tokenize[:-2])\n",
    "            if t in result:\n",
    "                result[t]['ann'] = filename\n",
    "            else:\n",
    "                result[t] = {'ann': filename}\n",
    "            pass\n",
    "        elif file_tokenize[-1] == 'txt':\n",
    "            t = ' '.join(file_tokenize[:-2])\n",
    "            if t in result:\n",
    "                result[t]['txt'] = filename\n",
    "            else:\n",
    "                result[t] = {'txt': filename}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_annotation(annotation: str) -> List[str]:\n",
    "    types_dict = {\n",
    "        'ПЕРС': 'PERS',\n",
    "        'ЛОК': 'LOC',\n",
    "        'ОРГ': 'ORG',\n",
    "        'РІЗН': 'MISC',\n",
    "        'PERS': 'PERS',\n",
    "        'LOC': 'LOC',\n",
    "        'ORG': 'ORG',\n",
    "        'MISC': 'MISC',\n",
    "    }\n",
    "    ann_list = annotation.split('\\t')\n",
    "\n",
    "    type_annotation = types_dict[ann_list[1].split(' ')[0]]\n",
    "    tokens = tokenize_uk.tokenize_words(ann_list[2])\n",
    "    return list(map(lambda x: (x, f'{type_annotation}'), tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_text_to_parallel() -> pd.DataFrame:\n",
    "    df = pd.DataFrame()\n",
    "    for filename, paths in tqdm(get_filenames().items()):\n",
    "        with open(PATH2COURPUS / paths['txt'], 'r') as file:\n",
    "            txt = file.read()\n",
    "\n",
    "        with open(PATH2COURPUS / paths['ann'], 'r') as file:\n",
    "            ann = file.read()\n",
    "\n",
    "        tokens = list()\n",
    "        list(\n",
    "            map(\n",
    "                lambda x: tokens.extend(process_annotation(x)) if x else x,\n",
    "                ann.split('\\n'),\n",
    "            )\n",
    "        )\n",
    "        txt = map(\n",
    "            lambda sentence: tokenize_uk.tokenize_words(sentence),\n",
    "            tokenize_uk.tokenize_sents(' '.join(tokenize_uk.tokenize_words(txt))),\n",
    "        )\n",
    "\n",
    "        for sentence in txt:\n",
    "            words = []\n",
    "            tags = []\n",
    "            for word in sentence:\n",
    "                if tokens and word == tokens[0][0]:\n",
    "                    word, tag = tokens.pop(0)\n",
    "                    words.append(word)\n",
    "                    tags.append(tag)\n",
    "                else:\n",
    "                    words.append(word)\n",
    "                    tags.append('O')\n",
    "\n",
    "        df = df.append([{'filename': filename, 'text': words, 'tags': tags}])\n",
    "    return df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 264/264 [00:01<00:00, 211.46it/s]\n"
     ]
    }
   ],
   "source": [
    "# result = raw_text_to_tabular()\n",
    "result = raw_text_to_parallel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_pickle(PATH2ROOT / CONFIG['data']['path_to_preproc_data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.read_pickle(PATH2ROOT / CONFIG['data']['path_to_preproc_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'LOC', 'O', 'O'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.iloc[1:2]['tags'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
