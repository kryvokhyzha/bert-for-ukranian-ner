{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings and useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config(filename: Path) -> Dict:\n",
    "    with open(filename, 'r') as file:\n",
    "        config = yaml.load(file, Loader=yaml.FullLoader)\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH2ROOT = Path('..')\n",
    "PATH2CONFIG = Path(PATH2ROOT / 'configs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = get_config(PATH2CONFIG / 'config.yml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH2COURPUS = Path(PATH2ROOT / CONFIG['data']['path_to_corpus_folder'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filenames() -> Dict[str, Dict[str, str]]:\n",
    "    result = dict()\n",
    "    for filename in os.listdir(PATH2COURPUS):\n",
    "        file_tokenize = filename.split('.')\n",
    "\n",
    "        if file_tokenize[-2] != 'tok':\n",
    "            continue\n",
    "        elif file_tokenize[-1] == 'ann':\n",
    "            t = ' '.join(file_tokenize[:-2])\n",
    "            if t in result:\n",
    "                result[t]['ann'] = filename\n",
    "            else:\n",
    "                result[t] = {'ann': filename}\n",
    "            pass\n",
    "        elif file_tokenize[-1] == 'txt':\n",
    "            t = ' '.join(file_tokenize[:-2])\n",
    "            if t in result:\n",
    "                result[t]['txt'] = filename\n",
    "            else:\n",
    "                result[t] = {'txt': filename}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_text_to_tabular() -> pd.DataFrame:\n",
    "    df = pd.DataFrame(columns=['word', 'tag'])\n",
    "    for filename, paths in tqdm(get_filenames().items()):\n",
    "        try:\n",
    "            ann = (\n",
    "                pd.read_csv(PATH2COURPUS / paths['ann'], sep='\\t', header=None)\n",
    "                .dropna()\n",
    "                .reset_index(drop=True)\n",
    "            )\n",
    "        except pd.errors.EmptyDataError:\n",
    "            continue\n",
    "        ann.columns = ['index', 'tag', 'phrase']\n",
    "\n",
    "        with open(PATH2COURPUS / paths['txt'], 'r') as file:\n",
    "            txt = file.read()\n",
    "\n",
    "        parallel_txt = txt\n",
    "\n",
    "        for row in ann.values:\n",
    "            tag = row[1].split()[0]\n",
    "            phrase = row[2]\n",
    "            n_words_in_phrase = len(phrase.split())\n",
    "            parallel_txt = parallel_txt.replace(\n",
    "                phrase, ' '.join([tag] * n_words_in_phrase), 1\n",
    "            )\n",
    "\n",
    "        assert len(parallel_txt.split()) == len(txt.split())\n",
    "\n",
    "        sublist = []\n",
    "        for word, parallel_word in zip(txt.split(), parallel_txt.split()):\n",
    "            if word == parallel_word:\n",
    "                sublist.append({'filename': filename, 'word': word, 'tag': 'None'})\n",
    "            else:\n",
    "                sublist.append({'filename': filename, 'word': word, 'tag': parallel_word})\n",
    "        df = df.append(sublist)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 264/264 [00:02<00:00, 95.83it/s] \n"
     ]
    }
   ],
   "source": [
    "result = raw_text_to_tabular()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_pickle(PATH2ROOT / CONFIG['data']['path_to_preproc_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
