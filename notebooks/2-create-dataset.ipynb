{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings and useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config(filename: Path) -> Dict:\n",
    "    with open(filename, 'r') as file:\n",
    "        config = yaml.load(file, Loader=yaml.FullLoader)\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH2ROOT = Path('..')\n",
    "PATH2CONFIG = Path(PATH2ROOT / 'configs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = get_config(PATH2CONFIG / 'config.yml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH2COURPUS = Path(PATH2ROOT / CONFIG['data']['path_to_corpus_folder'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = joblib.load(PATH2ROOT / CONFIG['data']['path_to_preproc_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'youscan/ukr-roberta-base'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Iterable, Mapping, Union\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "class NamedEntityRecognitionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for NER task.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        texts: Iterable[Iterable[str]],\n",
    "        tags: Iterable[Iterable[str]] = None,\n",
    "        tags_dict: Mapping[str, int] = None,\n",
    "        tokenizer: Union[\n",
    "            str, transformers.tokenization_utils.PreTrainedTokenizer\n",
    "        ] = 'distilbert-base-uncased',\n",
    "        max_seq_len: int = None,\n",
    "        lazy_mode: bool = True,\n",
    "    ):\n",
    "        self.tags = tags\n",
    "        self.texts = texts\n",
    "        self.tags_dict = tags_dict\n",
    "\n",
    "        if isinstance(tokenizer, str):\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(tokenizer)\n",
    "        elif isinstance(tokenizer, transformers.tokenization_utils.PreTrainedTokenizer):\n",
    "            self.tokenizer = tokenizer\n",
    "        else:\n",
    "            raise TypeError(\n",
    "                \"You pass wrong type of tokenizer. It should be a model name or PreTrainedTokenizer.\"\n",
    "            )\n",
    "\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.length = len(texts)\n",
    "\n",
    "        if self.max_seq_len < 3:\n",
    "            raise ValueError(\"Max sequence length should be greather than 2\")\n",
    "\n",
    "        if self.tags_dict is None and tags is not None:\n",
    "            # {'class1': 0, 'class2': 1, 'class3': 2, ...}\n",
    "            # using this instead of `sklearn.preprocessing.LabelEncoder`\n",
    "            # no easily handle unknown target values\n",
    "            self.tags_dict = dict(\n",
    "                zip(\n",
    "                    sorted(set([item for sublist in tags for item in sublist])),\n",
    "                    range(len(set([item for sublist in tags for item in sublist]))),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if not lazy_mode:\n",
    "            pbar = tqdm(self.length, desc=\"tokenizing texts\")\n",
    "            self.encoded = [self._getitem_lazy(idx) for idx in pbar]\n",
    "            del self.texts\n",
    "            del self.tags\n",
    "\n",
    "        self._getitem_fn = self._getitem_lazy if lazy_mode else self._getitem_encoded\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.length\n",
    "\n",
    "    def _getitem_encoded(self, index: int) -> Dict[str, torch.Tensor]:\n",
    "        return torch.tensor(self.encoded[index])\n",
    "\n",
    "    def _getitem_lazy(self, index: int) -> Dict[str, torch.Tensor]:\n",
    "        sentence = self.texts[index]\n",
    "        tag = self.tags[index]\n",
    "\n",
    "        input_ids = []\n",
    "        target_tag = []\n",
    "\n",
    "        for i, word in enumerate(sentence):\n",
    "            words_piece_ids = self.tokenizer.encode(\n",
    "                word,\n",
    "                max_length=self.max_seq_len,\n",
    "                truncation=True,\n",
    "                add_special_tokens=False,\n",
    "            )\n",
    "            input_ids.extend(words_piece_ids)\n",
    "            if self.tags is not None:\n",
    "                target_tag.extend([tag[i]] * len(words_piece_ids))\n",
    "\n",
    "        input_ids = [101] + input_ids[: self.max_seq_len - 2] + [102]\n",
    "\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        token_type_ids = [0] * len(input_ids)\n",
    "\n",
    "        padding_len = self.max_seq_len - len(input_ids)\n",
    "        input_ids = input_ids + ([0] * padding_len)\n",
    "        attention_mask = attention_mask + ([0] * padding_len)\n",
    "        token_type_ids = token_type_ids + ([0] * padding_len)\n",
    "\n",
    "        if self.tags is not None:\n",
    "            target_tag = [self.tags_dict.get(y, -1) for y in target_tag]\n",
    "            target_tag = [0] + target_tag[: self.max_seq_len - 2] + [0]\n",
    "            target_tag = target_tag + ([0] * padding_len)\n",
    "\n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'target_tag': torch.tensor(target_tag, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "    def __getitem__(self, index: int) -> Dict[str, torch.Tensor]:\n",
    "        return self._getitem_fn(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = data['text'].values.tolist()\n",
    "tags = data['tags'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = NamedEntityRecognitionDataset(\n",
    "    texts, tags=tags, tokenizer=MODEL_NAME, max_seq_len=16, lazy_mode=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([  101,  4293,  2097,  7739, 21309,   275,   333,   630,  7892,   324,\n",
       "           431,  2364, 14572,    18,   102,     0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'target_tag': tensor([0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0])}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LOC': 0, 'MISC': 1, 'O': 2, 'ORG': 3, 'PERS': 4}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.tags_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
