{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings and useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config(filename: Path) -> Dict:\n",
    "    with open(filename, 'r') as file:\n",
    "        config = yaml.load(file, Loader=yaml.FullLoader)\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH2ROOT = Path('..')\n",
    "PATH2CONFIG = Path(PATH2ROOT / 'configs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = get_config(PATH2CONFIG / 'config.yml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH2COURPUS = Path(PATH2ROOT / CONFIG['data']['path_to_corpus_folder'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filenames() -> Dict[str, Dict[str, str]]:\n",
    "    result = dict()\n",
    "    for filename in os.listdir(PATH2COURPUS):\n",
    "        file_tokenize = filename.split('.')\n",
    "\n",
    "        if file_tokenize[-2] != 'tok':\n",
    "            continue\n",
    "        elif file_tokenize[-1] == 'ann':\n",
    "            t = ' '.join(file_tokenize[:-2])\n",
    "            if t in result:\n",
    "                result[t]['ann'] = filename\n",
    "            else:\n",
    "                result[t] = {'ann': filename}\n",
    "            pass\n",
    "        elif file_tokenize[-1] == 'txt':\n",
    "            t = ' '.join(file_tokenize[:-2])\n",
    "            if t in result:\n",
    "                result[t]['txt'] = filename\n",
    "            else:\n",
    "                result[t] = {'txt': filename}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_text_to_tabular() -> pd.DataFrame:\n",
    "    df = pd.DataFrame()\n",
    "    for filename, paths in tqdm(get_filenames().items()):\n",
    "        try:\n",
    "            ann = (\n",
    "                pd.read_csv(PATH2COURPUS / paths['ann'], sep='\\t', header=None)\n",
    "                .dropna()\n",
    "                .reset_index(drop=True)\n",
    "            )\n",
    "        except pd.errors.EmptyDataError:\n",
    "            continue\n",
    "        ann.columns = ['index', 'tag', 'phrase']\n",
    "\n",
    "        with open(PATH2COURPUS / paths['txt'], 'r') as file:\n",
    "            txt = file.read()\n",
    "\n",
    "        parallel_txt = txt\n",
    "\n",
    "        for row in ann.values:\n",
    "            tag = row[1].split()[0]\n",
    "            phrase = row[2]\n",
    "            n_tokens_in_phrase = len(phrase.split())\n",
    "            parallel_txt = parallel_txt.replace(\n",
    "                phrase, ' '.join([tag] * n_tokens_in_phrase), 1\n",
    "            )\n",
    "\n",
    "        #         txt_split = txt.split()\n",
    "        #         for idx, token in enumerate(txt_split):\n",
    "        #             if token in ABBREVIATIONS and idx < len(txt_split)-1 and txt_split[idx+1] in DELIMITERS:\n",
    "        #                 pass\n",
    "        #         del txt_split\n",
    "\n",
    "        assert len(parallel_txt.split()) == len(txt.split())\n",
    "\n",
    "        sublist = []\n",
    "        for idx, (sentence, parallel_sentence) in enumerate(\n",
    "            zip(txt.split('\\n'), parallel_txt.split('\\n'))\n",
    "        ):\n",
    "            if sentence == '':\n",
    "                continue\n",
    "\n",
    "            assert len(sentence.split()) == len(parallel_sentence.split())\n",
    "            for token, parallel_token in zip(sentence.split(), parallel_sentence.split()):\n",
    "                if token == parallel_token:\n",
    "                    sublist.append(\n",
    "                        {\n",
    "                            'filename': filename,\n",
    "                            'sentence': f'sentence {idx+1}',\n",
    "                            'token': token,\n",
    "                            'tag': 'None',\n",
    "                        }\n",
    "                    )\n",
    "                else:\n",
    "                    sublist.append(\n",
    "                        {\n",
    "                            'filename': filename,\n",
    "                            'sentence': f'sentence {idx+1}',\n",
    "                            'token': token,\n",
    "                            'tag': parallel_token,\n",
    "                        }\n",
    "                    )\n",
    "        df = df.append(sublist)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 264/264 [00:03<00:00, 72.86it/s]\n"
     ]
    }
   ],
   "source": [
    "result = raw_text_to_tabular()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_pickle(PATH2ROOT / CONFIG['data']['path_to_preproc_data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.read_pickle(PATH2ROOT / CONFIG['data']['path_to_preproc_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>sentence</th>\n",
       "      <th>token</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A_Halytskyi_korespondent_Fedoliak_Baseyn_peret...</td>\n",
       "      <td>sentence 1</td>\n",
       "      <td>Не</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A_Halytskyi_korespondent_Fedoliak_Baseyn_peret...</td>\n",
       "      <td>sentence 1</td>\n",
       "      <td>встигла</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A_Halytskyi_korespondent_Fedoliak_Baseyn_peret...</td>\n",
       "      <td>sentence 1</td>\n",
       "      <td>новостворена</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A_Halytskyi_korespondent_Fedoliak_Baseyn_peret...</td>\n",
       "      <td>sentence 1</td>\n",
       "      <td>П’ядицька</td>\n",
       "      <td>LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A_Halytskyi_korespondent_Fedoliak_Baseyn_peret...</td>\n",
       "      <td>sentence 1</td>\n",
       "      <td>ОТГ</td>\n",
       "      <td>LOC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filename    sentence  \\\n",
       "0  A_Halytskyi_korespondent_Fedoliak_Baseyn_peret...  sentence 1   \n",
       "1  A_Halytskyi_korespondent_Fedoliak_Baseyn_peret...  sentence 1   \n",
       "2  A_Halytskyi_korespondent_Fedoliak_Baseyn_peret...  sentence 1   \n",
       "3  A_Halytskyi_korespondent_Fedoliak_Baseyn_peret...  sentence 1   \n",
       "4  A_Halytskyi_korespondent_Fedoliak_Baseyn_peret...  sentence 1   \n",
       "\n",
       "          token   tag  \n",
       "0            Не  None  \n",
       "1       встигла  None  \n",
       "2  новостворена  None  \n",
       "3     П’ядицька   LOC  \n",
       "4           ОТГ   LOC  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
